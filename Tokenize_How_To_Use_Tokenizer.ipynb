{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tokenize_How_To_Use_Tokenizer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPmqwHj6BTa/EFhNR/EFAJu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6meSRsLrtNs","executionInfo":{"status":"ok","timestamp":1649461984330,"user_tz":300,"elapsed":29297,"user":{"displayName":"Sterling Miller","userId":"05467034614136563673"}},"outputId":"496e7dab-e028-4c29-d0c4-4eddf6de0049"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Fetched 252 kB in 2s (101 kB/s)\n","Reading package lists... Done\n"]}],"source":["import os\n","# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark verion\n","# For example:\n","# spark_version = 'spark-3.<enter version>'\n","spark_version = 'spark-3.2.1'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n","!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","# Set Environment Variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()"]},{"cell_type":"code","source":["# Start Spark session\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"Tokens\").getOrCreate()"],"metadata":{"id":"JH40brbWSl2E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer"],"metadata":{"id":"vVHyZLfITp9Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create sample DataFrame from scratch\n","dataframe = spark.createDataFrame([\n","                                   (0, \"Spark is great\"),\n","                                   (1, \"We are learning Spark\"),\n","                                   (2, \"Spark is better than hadoop no doubt\")\n","], [\"id\", \"sentence\"])\n","\n","dataframe.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbdfNHXpT2z8","executionInfo":{"status":"ok","timestamp":1649457628527,"user_tz":300,"elapsed":6416,"user":{"displayName":"Sterling Miller","userId":"05467034614136563673"}},"outputId":"c872d56c-b693-4a1e-ef42-f228ea1b8182"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+--------------------+\n","| id|            sentence|\n","+---+--------------------+\n","|  0|      Spark is great|\n","|  1|We are learning S...|\n","|  2|Spark is better t...|\n","+---+--------------------+\n","\n"]}]},{"cell_type":"code","source":["# The tokenizer function takes input and output parameters. \n","# The input passes the name of the column that we want to \n","# have tokenized, and the output takes the name that we want the column called.\n","\n","\n","\n","# Tokenizer sentences\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","tokenizer\n","# ^ tokenizer by itself is only a transform method, so \n","# when we call it alone it won't show us the dataframe\n","\n","\n","\n","# To see the dataframe, tokenizer uses a transform function to take in DataFrames as inputs\n","tokenized_df = tokenizer.transform(dataframe)\n","# Then we need to use a .show() function\n","# .show(truncate=False) shows the tokenized dataframe without shortening the output\n","tokenized_df.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhL72rdMT24H","executionInfo":{"status":"ok","timestamp":1649458866094,"user_tz":300,"elapsed":1341,"user":{"displayName":"Sterling Miller","userId":"05467034614136563673"}},"outputId":"69000b36-d3cb-40f2-966c-1cc17b06bca6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------------------------+--------------------------------------------+\n","|id |sentence                            |words                                       |\n","+---+------------------------------------+--------------------------------------------+\n","|0  |Spark is great                      |[spark, is, great]                          |\n","|1  |We are learning Spark               |[we, are, learning, spark]                  |\n","|2  |Spark is better than hadoop no doubt|[spark, is, better, than, hadoop, no, doubt]|\n","+---+------------------------------------+--------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["# User-Defined Functions (UDFs): Functions created by the user to add custom output columns\n","# Next, we'll want to create a function to enhance our tokenizer\n"],"metadata":{"id":"HRvt8sf7YbOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's add a function that returns a word count for each line\n","\n","# Start by creating a Python function that takes a list of words as its input, \n","# then returns the length of that list\n","\n","\n","\n","# Create a function to return the length of a list\n","def word_list_length(word_list):\n","  return len(word_list)\n","\n","\n","\n","# Test the function\n","word_list_0 = [\"testing\", \"this\", \"function\", \"out\", \"using\", \"these\", \"words\"]\n","word_list_length(word_list_0)\n","\n","\n","\n","# # Another Function - NOT AS ELEGANT, NOT AS CONVENIENT\n","# def word_list_length(word_list):\n","#   x = 0\n","#   for word in word_list:\n","#     x += 1\n","#   return x\n","\n","# # Test function\n","# word_list_1 = [\"spark\", \"is\", \"great\"]\n","# word_list_length(word_list_1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lnf9I9W6T3AN","executionInfo":{"status":"ok","timestamp":1649460624245,"user_tz":300,"elapsed":153,"user":{"displayName":"Sterling Miller","userId":"05467034614136563673"}},"outputId":"1a8934eb-d243-4561-b3ac-d653ee45bfd3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# Next we'll need to import the udf function from pyspark to create User-Defined Functions\n","# We'll also want to import the col function to select a column to be passed into a function\n","from pyspark.sql.functions import col, udf\n","\n","# And finally, we'll want to import the IntergerType function so we can define datatype outputs in udf functions\n","from pyspark.sql.types import IntegerType"],"metadata":{"id":"RnRw2bl7g8Qb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a user defined function - a function that can add data output columns directly into dataframes\n","count_tokens = udf(word_list_length, IntegerType())"],"metadata":{"id":"58v9MkF-T3QM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we need to redo the tokenizer process\n","\n","# Create out Tokenizer\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","\n","# Transform DataFrame\n","tokenized_df = tokenizer.transform(dataframe)\n","\n","# Select the needed columns and don't truncate results\n","tokenized_df.withColumn(\"tokens\", count_tokens(col(\"words\"))).show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1FmR84AXjw-h","executionInfo":{"status":"ok","timestamp":1649461514601,"user_tz":300,"elapsed":2006,"user":{"displayName":"Sterling Miller","userId":"05467034614136563673"}},"outputId":"9530882b-fb1b-4f2b-d883-ac438c06f485"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------------------------+--------------------------------------------+------+\n","|id |sentence                            |words                                       |tokens|\n","+---+------------------------------------+--------------------------------------------+------+\n","|0  |Spark is great                      |[spark, is, great]                          |3     |\n","|1  |We are learning Spark               |[we, are, learning, spark]                  |4     |\n","|2  |Spark is better than hadoop no doubt|[spark, is, better, than, hadoop, no, doubt]|7     |\n","+---+------------------------------------+--------------------------------------------+------+\n","\n"]}]}]}